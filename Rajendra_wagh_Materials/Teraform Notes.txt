***********************************

Terraform Providers â€” A Terraform Provider is a plugin that lets Terraform interact with cloud platforms and services like AWS, Azure, GCP, GitHub, MySQL, Kubernetes, etc.

Think of a provider as a bridge between Terraform and the service you want to manage.

â­ What Providers Do

A provider is responsible for:

Authentication (login to AWS, Azure, etc.)

API interactions (create/update/delete resources)

Managing resource types (EC2, S3, VPC, etc.)

Data sources (fetch info from the platform)

ğŸ”§ How to Use a Provider (Example: AWS)
1. Declare the provider
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 6.0"
    }
  }
}

2. Configure the provider
provider "aws" {
  region = "ap-south-1"
}

ğŸ“¦ Common Terraform Providers
Provider	Purpose
aws		Manage AWS cloud resources
azurerm		Manage Azure resources
google		Manage GCP resources
kubernetes	Manage Kubernetes cluster resources
helm		Install Helm charts on Kubernetes
vault		Manage HashiCorp Vault
github		Manage GitHub repositories & teams
mysql		Manage MySQL databases

ğŸ” Provider Versions

Terraform allows version constraints:

Version syntax	Meaning
= 6.0.0	Exact version
>= 6.0	Minimum version
<= 6.0	Maximum version
~> 6.0	Compatible with 6.x (but not 7.x)
~> 6.1	Compatible with 6.1.x

Example:

version = ">= 5.0, < 7.0"

******************************************************************************
Terraform Providers Registry

The Terraform Registry is the official public website that stores and distributes providers, modules, and documentation for Terraform.

It is available at:
ğŸ‘‰ registry.terraform.io
*****************************************************************************
TLS provider

The TLS provider is used to create and manage cryptographic materials such as:

1. Private keys
2. Public keys
3. TLS certificates
4. Self-signed certificates

It is not related to cloud providers like AWS or Azure.
Instead, it helps you generate security assets locally using Terraform.

ğŸ“¦ Why Use the TLS Provider?

You use the TLS provider when you need to generate:

âœ” SSH keys
âœ” TLS certificates
âœ” Privateâ€“public key pairs
âœ” Self-signed certificates
âœ” Certificate signing requests (CSR)

Example use cases:

Creating SSH key pairs for EC2 instances
Generating TLS certificates for Kubernetes clusters
Creating certificates for internal services

How to Declare the TLS Provider
terraform {
  required_providers {
    tls = {
      source  = "hashicorp/tls"
      version = "~> 4.0"
    }
  }
}

Most Important TLS Provider Resources
1. tls_private_key

Generates a privateâ€“public key pair.

2. tls_self_signed_cert

Creates a self-signed certificate.

3. tls_cert_request

Creates a CSR (Certificate Signing Request).

4. tls_locally_signed_cert

Signs a CSR using another private key.


Generate an SSH key pair

resource "tls_private_key" "ssh_key" {
  algorithm = "RSA"
  rsa_bits  = 4096
}

output "public_key" {
  value = tls_private_key.ssh_key.public_key_openssh
}

output "private_key_pem" {
  value     = tls_private_key.ssh_key.private_key_pem
  sensitive = true
}


The key pair exists only in Terraform state and is shown in terraform output, not saved as .pem or .pub files.

Here is a complete working Terraform example that:

âœ… Generates SSH key pair
âœ… Saves public + private keys to local files
âœ… Imports the public key into AWS EC2 as an AWS Key Pair
âœ… Uses the key pair to launch an EC2 instance

This is the correct & recommended approach.

ğŸš€ FULL WORKING TERRAFORM EXAMPLE
1. Generate SSH Key Pair
resource "tls_private_key" "ssh_key" {
  algorithm = "RSA"
  rsa_bits  = 4096
}

2. Save Keys to Local Files
âœ” Public key file
resource "local_file" "public_key" {
  filename       = "${path.module}/id_rsa.pub"
  content_base64 = base64encode(tls_private_key.ssh_key.public_key_openssh)
}

âœ” Private key file
resource "local_sensitive_file" "private_key" {
  filename = "${path.module}/id_rsa"
  content  = tls_private_key.ssh_key.private_key_pem
}


These files will appear in your Terraform directory:

id_rsa      â†’ private key
id_rsa.pub  â†’ public key

3. Create AWS EC2 Key Pair using the public key
resource "aws_key_pair" "mykey" {
  key_name   = "my-terraform-key"
  public_key = tls_private_key.ssh_key.public_key_openssh
}

4. Launch an EC2 Instance using this key
resource "aws_instance" "my_ec2" {
  ami           = "ami-0123456789abcdef0"   # <-- Replace with your AMI
  instance_type = "t2.micro"

  key_name = aws_key_pair.mykey.key_name

  tags = {
    Name = "MyEC2"
  }
}

âœ” What Happens When You Apply
After:
terraform init
terraform apply


Terraform will:

âœ” Generate key pair
âœ” Save to disk:
./id_rsa
./id_rsa.pub

âœ” Upload public key to AWS EC2
âœ” Launch an EC2 using that key
âœ” Private key on your machine = used for SSH login

**********************************************************
Terraform Formatting â€” terraform fmt

Terraform provides a built-in formatter to keep your .tf files clean, consistent, and readable.

The command used is:

terraform fmt

ğŸ§¹ What terraform fmt Does

When you run this command, Terraform:

âœ” Fixes indentation
âœ” Aligns equal signs (=)
âœ” Properly aligns arguments
âœ” Corrects spacing
âœ” Rewrites code following Terraform style conventions
âœ” Updates nested blocks
âœ” Normalizes list and map formatting

This helps maintain code quality across your team.

ğŸ‘‰ Basic Usage
Format the current directory:
terraform fmt

Format a specific file:
terraform fmt main.tf

Example â€” Before & After Formatting
Before (messy code)
resource "aws_s3_bucket"    "demo" {
bucket="my-bucket"

  tags={Name="DemoBucket"}
}

After running terraform fmt
resource "aws_s3_bucket" "demo" {
  bucket = "my-bucket"

  tags = {
    Name = "DemoBucket"
  }
}

**********************************************

Terraform Debug Logging (TF_LOG)

Terraform has built-in logging that you can enable using the TF_LOG environment variable.

Available log levels
Level	Use
TRACE	Most detailed logs (API calls, internal functions)
DEBUG	Debug information
INFO	Normal useful logs
WARN	Warnings
ERROR	Only errors


Enable Logging Temporarily (One Command)
TF_LOG=DEBUG terraform apply

ğŸ“‚ 2. Save Logs to a File (Recommended)

You can store all logs to a file using TF_LOG_PATH.

export TF_LOG=TRACE
export TF_LOG_PATH=terraform.log
terraform plan
terraform apply

This creates a file called terraform.log with full details.

ğŸ“ 3. Disable Logging
unset TF_LOG
unset TF_LOG_PATH

ğŸ”Œ 4. Provider-Specific Logs (AWS example)

Some providers support their own logging.

AWS Provider Logging:
export AWS_LOG_LEVEL=debug
export AWS_SDK_LOAD_CONFIG=1

*************************************************
Terraform Workflow (Standard Lifecycle)

Terraform follows a simple 4-step workflow, no matter what cloud you use:

1. Write main.tf
2. terraform Init
3. terraform plan
4. terraform apply
(Optional: terraform destroy)

*****************************************************************************

Terraform State â€” What It Is

Terraform uses a state file (terraform.tfstate) to keep track of:

âœ” The resources Terraform created
âœ” Their real IDs in the cloud
âœ” Resource dependencies
âœ” Metadata (tags, names, configs)

The state file is the single source of truth for Terraform.

ğŸ“Œ Why Does Terraform Need State?

Because Terraform must know:

What resources already exist

What needs to be created

What needs to be updated

What needs to be deleted

Dependency relationships

Without state, Terraform cannot work.

ğŸ“‚ Where State Is Stored
1. Local State (default)

Stored in your project folder:

terraform.tfstate
terraform.tfstate.backup

2. Remote State (recommended for teams)

Stored in:

AWS S3

Azure Blob

Google Cloud Storage

Terraform Cloud

Consul

ğŸ§  Why Remote State?

Because:

âœ” Enables team collaboration
âœ” Prevents conflicts
âœ” Provides locking (DynamoDB / GCS locking / Terraform Cloud locking)
âœ” Prevents state corruption
âœ” More secure (encrypted)

ğŸ”’ State Locking

Prevents two people running Terraform at the same time.

Example in AWS:

S3 backend stores state

DynamoDB table locks state

ğŸ› ï¸ Common State Commands
Show current state
terraform state list
*****************************************************
Terraform Import â€” What It Is

terraform import is used to bring existing resources (already created manually or outside Terraform) into Terraform state.

âš  Important:
Import only updates the Terraform state file â€” it does NOT create the .tf configuration automatically.
You still need to write the .tf code.

ğŸ§  Why Do We Use terraform import?

Use it when:

Resources already exist in AWS, Azure, GCP, etc.

You want Terraform to start managing them

You want to avoid deleting and recreating resources

Examples:

Import existing S3 bucket

Import EC2 instance

Import VPC

Import IAM roles

Import Kubernetes resources

ğŸ”§ General Syntax
terraform import <resource_address> <resource_id>

ğŸ§© 1. Example: Import an AWS S3 Bucket
Step 1 â€” Create the TF resource
resource "aws_s3_bucket" "demo" {}

Step 2 â€” Import into Terraform
terraform import aws_s3_bucket.demo my-existing-bucket-name


Now Terraform will manage that bucket.

ğŸ–¥ï¸ 2. Import an EC2 Instance
Write minimal configuration:
resource "aws_instance" "server" {}

Find Instance ID from AWS:
i-0aa12bb34cc56dd78

Import:
terraform import aws_instance.server i-0aa12bb34cc56dd78

ğŸ—ï¸ 3. Import a VPC
terraform import aws_vpc.main vpc-123abcd

ğŸ—‚ 4. Import AWS Subnet
terraform import aws_subnet.public_subnet subnet-ab12cd34

ğŸ“¦ 5. Import IAM Role

IAM uses the role name as ID:

terraform import aws_iam_role.myrole my-role-name

ğŸ’¡ Important Notes
âœ” Import does NOT create .tf code

You must write your own .tf configuration that matches the real resource.

âœ” Run terraform plan after import

It will show differences between your .tf file and the imported resource.

âœ” Use import for migration

If moving a manually created infra to Terraform.

*****************************************************************************

Backends

Terraform backends define where and how Terraform stores the state file (terraform.tfstate).

Think of a backend as the storage and locking system for Terraform state.

ğŸ§© Why Backends Matter

Terraform needs to store state safely because it tracks:

Resource IDs

Attributes

Dependencies

Outputs

A backend controls:

âœ” Where state file is stored
âœ” Who can access it
âœ” Whether locking is enabled
âœ” Whether remote runs are possible
ğŸ—ï¸ Two Types of Backends
1ï¸âƒ£ Local Backend (default)

State stored on your local machine

File: terraform.tfstate

No locking

Not safe for teams

Example:

(No configuration required â€” it's default)

2ï¸âƒ£ Remote Backends

These store state in a shared remote system.

Common backends:

Backend				State Storage				Locking				Notes
S3					S3 Bucket					Yes(with DynamoDB)	Most popular for AWS
Terraform Cloud		Cloud					Yes					Best for teams
Consul				Consul KV				Yes					Popular in HashiCorp stack
GCS					Google Cloud Storage		No					Needs external lock
Azure Blob Storage		Azure					Yes					Uses leases
Etcdv3				etcd						Yes					Kubernetes users
Artifactory			JFrog					Yes					enterprise
Local				Local disk					No					default


ğŸ“Œ Backend Example: 
Folder 1: Create Backend Resources (Local State)

terraform-backend/backend-resources.tf:

provider "aws" {
  region = "us-east-1"
}

resource "aws_s3_bucket" "tf_state" {
  bucket = "raj-terraform-state-bucket-001"

  versioning {
    enabled = true
  }

  server_side_encryption_configuration {
    rule {
      apply_server_side_encryption_by_default {
        sse_algorithm = "AES256"
      }
    }
  }
}

resource "aws_dynamodb_table" "tf_lock" {
  name         = "terraform-state-lock"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "LockID"

  attribute {
    name = "LockID"
    type = "S"
  }
}


Run:

cd terraform-backend
terraform init
terraform apply


ğŸ‘‰ This state will remain local forever.

ğŸŸ© Folder 2: Your Actual Project (Remote State)

project/main.tf:

terraform {
  backend "s3" {
    bucket         = "raj-terraform-state-bucket-1"
    key            = "project1/terraform.tfstate"
    region         = "us-east-1"
    dynamodb_table = "terraform-state-lock"
    encrypt        = true
  }
}

provider "aws" {
  region = "us-east-1"
}

resource "aws_s3_bucket" "demo" {
  bucket = "raj-demo-s3-bucket-1"
}


Then run:

cd ../project
terraform init

PART 3 â€” Full Backend Migration Example (Local â†’ S3)
ğŸ“Œ Step A â€” BEFORE migration (local backend)

Your main.tf originally:

provider "aws" {
  region = "us-east-1"
}

resource "aws_s3_bucket" "demo" {
  bucket = "local-backend-demo-123"
}


Run initial deploy:

terraform init
terraform apply


State is stored locally in:

terraform.tfstate

ğŸ“Œ Step B â€” Update configuration to use S3 backend

Modify main.tf:

terraform {
  backend "s3" {
    bucket         = "raj-terraform-state-bucket-001"
    key            = "project1/terraform.tfstate"
    region         = "us-east-1"
    dynamodb_table = "terraform-state-lock"
  }
}

provider "aws" {
  region = "us-east-1"
}

resource "aws_s3_bucket" "demo" {
  bucket = "local-backend-demo-123"
}

ğŸ“Œ Step C â€” Migrate state

Run:

terraform init


Message:

Terraform detected that the backend changed from "local" to "s3".
Do you want to copy your existing state to the new backend? (yes/no)


Type:

yes


Terraform moves terraform.tfstate â†’ into your S3 bucket.

ğŸ‰ Migration completed successfully!

âœ” Verify State Migration

Check S3 bucket:

You will see:

project1/terraform.tfstate


Check DynamoDB:

You will see lock records when running:

terraform plan

************************************Terraform: Working with Data Blocks

Terraform data blocks (also called data sources) are used to read or fetch existing information from your provider (AWS, Azure, etc.) without creating resources.
They allow Terraform to query existing infrastructure so that you can use those values inside your configuration.

âœ… Why use Data Blocks?

Use data blocks when you need:

âœ” Information about existing resources

Fetch the latest AMI ID

Fetch VPC ID / subnet IDs from existing AWS environment

Read an existing security group

Get secrets from AWS Secrets Manager

âœ” Dynamic values

Get value at runtime instead of hardcoding it

âœ” Dependency management

Use depends_on if you want Terraform to fetch data after a specific resource is created

ğŸ”¹ Terraform Data Block Syntax
data "<PROVIDER>_<TYPE>" "<NAME>" {
  # arguments/filters...
}


Then you reference it as:

data.<PROVIDER>_<TYPE>.<NAME>.<ATTRIBUTE>

Example: Get Latest Amazon Linux 2 AMI
provider "aws" {
  region = "us-east-1"
}

data "aws_ami" "amazon_linux" {
  most_recent = true

  filter {
    name   = "name"
    values = ["amzn2-ami-hvm-*-x86_64-gp2"]
  }

  owners = ["amazon"]
}

resource "aws_instance" "web" {
  ami           = data.aws_ami.amazon_linux.id
  instance_type = "t2.micro"
}

output "ami_used" {
  value = data.aws_ami.amazon_linux.id
}

Example: Data Block for VPC
provider "aws" {
  region = "us-east-1"
}

data "aws_vpc" "default" {
  default = true
}

resource "aws_subnet" "demo" {
  vpc_id            = data.aws_vpc.default.id
  cidr_block        = "172.31.120.0/24"
  availability_zone = "us-east-1a"
}

output "default_vpc_id" {
  value = data.aws_vpc.default.id
}




# Read existing Default Security Group
provider "aws" {
  region = "us-east-1"
}
data "aws_security_group" "sg" {
  name = "default"
}

resource "aws_instance" "web" {
  ami           = "ami-0c02fb55956c7d316"
  instance_type = "t2.micro"

  vpc_security_group_ids = [data.aws_security_group.sg.id]
}

output "used_security_group" {
  value = data.aws_security_group.sg.id
}
******************************************************************

Terraform Built-In Functions

Terraform provides many built-in functions to transform, calculate, format, or query values inside your .tf code.

Functions are grouped into categories:

1. Numeric Functions
2. String Functions
3. Collection Functions
4. Encoding Functions
5. Filesystem Functions
6. CIDR/Network Functions
7. Type Conversion Functions
8. Date & Time Functions
9. Crypto Functions
10. Helpers (lookup, coalesce, try, can, etc.)

ğŸ”¶ 1. Numeric Functions
min() / max()
min(5, 4, 10)   # 4
max(5, 4, 10)   # 10

abs()
abs(-9)  # 9

ceil() / floor()
ceil(4.3)  # 5
floor(4.9) # 4

ğŸ”¶ 2. String Functions
upper() / lower()
upper("aws")   # "AWS"
lower("TERRAFORM") # "terraform"

trimspace()
trimspace("   hello   ") # "hello"

replace()
replace("ami-123", "123", "456") # "ami-456"

format()
format("Hello %s", "Raj")  # "Hello Raj"

join() / split()
join(",", ["a", "b", "c"])   # "a,b,c"
split(",", "a,b,c")          # ["a", "b", "c"]

ğŸ”¶ 3. Collection (List, Map) Functions
length()
length(["a", "b", "c"])  # 3

contains()
contains(["dev","qa","prod"], "dev") # true

lookup()
lookup({env="dev", region="us-east-1"}, "region")  # "us-east-1"

keys() / values()
keys({a=1,b=2})   # ["a","b"]
values({a=1,b=2}) # [1,2]

merge() (combine maps)
merge(
  {a=1, b=2},
  {b=3, c=4}
)
# {a=1, b=3, c=4}

zipmap()
zipmap(["name","age"], ["Raj","40"])
# {name="Raj", age="40"}

ğŸ”¶ 4. CIDR / Network Functions
cidrsubnet()

Used to calculate subnet CIDRs.

cidrsubnet("10.0.0.0/16", 4, 1)
# "10.0.16.0/20"

cidrhost()
cidrhost("10.0.0.0/24", 5)
# "10.0.0.5"

cidrnetmask()
ğŸ”¶ 5. Encoding Functions
jsonencode()
jsonencode({
  name = "Raj"
  env  = "dev"
})


Result:

{"name":"Raj","env":"dev"}

jsondecode()
ğŸ”¶ 6. Filesystem Functions
file()

Reads a file:

file("userdata.sh")

templatefile()
templatefile("${path.module}/script.tpl", {
  name = "Raj"
})

ğŸ”¶ 7. Date & Time Functions
timestamp()
timestamp()  # "2025-11-16T12:12:30Z"

formatdate()
formatdate("YYYY-MM-DD", timestamp())

ğŸ”¶ 8. Type Conversion Functions
tostring(), tonumber(), tolist(), tomap()
tonumber("5")  # 5
tostring(123)  # "123"

ğŸ”¶ 9. Crypto / Hash Functions
md5()
md5("hello")  

sha1(), sha256()
ğŸ”¶ 10. Helper Functions (VERY IMPORTANT)
coalesce()

Returns first non-null:

coalesce(null, "", "dev", "prod")  # ""

ğŸ§ª Practical Examples Youâ€™ll Use Every Day
1. Create a dynamic EC2 Name using functions
tags = {
  Name = format("web-%s-%s", var.env, timestamp())
}

What this does
âœ” format()

Works like printf in programming:

format("pattern", arg1, arg2, ...)

âœ” timestamp()

Returns the current UTC timestamp like:

2025-11-16T12:55:33Z


So if:

var.env = "dev"


Then the final EC2 Name tag becomes:

web-dev-2025-11-16T12:55:33Z

*************************************************************
What is Terraform Graph?

terraform graph generates a visual dependency graph of your Terraform resources.

Terraform internally creates a DAG (Directed Acyclic Graph) to decide in which order resources should be created, updated, or destroyed.

terraform graph allows you to see this graph.

ğŸ“Œ Basic Command
terraform graph


This outputs a .dot format graph (Graphviz format).

Example output:

"aws_vpc.main" -> "aws_subnet.public"
"aws_subnet.public" -> "aws_instance.web"

Graph in readable form using Graphviz

The output from terraform graph is not directly viewable â€” you must pipe it to Graphviz.

Install Graphviz

Ubuntu:

sudo apt install graphviz


Windows (Chocolatey):

choco install graphviz

ğŸ“Œ Generate visual graph image
PNG output
terraform graph | dot -Tpng > graph.png

SVG output
terraform graph | dot -Tsvg > graph.svg


Example Dependency Flow

Imagine you have:

VPC

Subnet

Security Group

EC2 Instance

Terraform graph will show:

aws_vpc.main
  â†“
aws_subnet.public
  â†“
aws_security_group.web_sg
  â†“
aws_instance.web

*************************************************

Security in Terraform (Complete Guide)

Terraform interacts with cloud providers and stores sensitive values in its state file, so security is critical.
Letâ€™s break Terraform security into key areas ğŸ‘‡

âœ… 1. Secure Terraform State File

Terraform stores everything in terraform.tfstate, including:

passwords

secret keys

DB connection strings

private IPs

AMI IDs

user_data scripts

Never store state locally for production

Use a remote backend:

Best secure backends:

âœ” Terraform Cloud
âœ” AWS S3 + DynamoDB (most common)
âœ” Azure Blob Storage
âœ” GCS Bucket

Example (Secure remote backend in AWS):
terraform {
  backend "s3" {
    bucket         = "tf-state-bucket"
    key            = "prod/terraform.tfstate"
    region         = "us-east-1"
    dynamodb_table = "terraform-lock"
    encrypt        = true
  }
}

Important Security Notes:

Enable S3 bucket encryption (AES-256 or KMS)

Enable S3 versioning

Block public access

Use IAM policy to restrict access

DynamoDB table for state locking

âœ… 2. Secure Secrets and Sensitive Values

Never hard-code secrets in Terraform files.

Use variables marked as sensitive = true:
variable "db_password" {
  type      = string
  sensitive = true
}


Terraform will hide it from:

console output

logs

plan/apply output

Better secret sources:

âœ” AWS Secrets Manager
âœ” AWS SSM Parameter Store
âœ” Vault
âœ” Azure Key Vault

Example:

data "aws_ssm_parameter" "db_pass" {
  name = "/prod/db/password"
  with_decryption = true
}

âœ… 3. Secure Provider Credentials

Avoid using:

âœ– Hard-coded AWS access key
âœ– Credentials in .tf files
âœ– Credentials in code repo

Use secure methods instead:

âœ” AWS CLI with aws configure
âœ” Environment variables
âœ” Vault
âœ” EC2 Instance Role (best)
âœ” SSO / IAM Identity Center (best for enterprise)

Example:

export AWS_PROFILE=prod


Or:

export AWS_ACCESS_KEY_ID=xxxx
export AWS_SECRET_ACCESS_KEY=xxxx

âœ… 4. IAM Security Best Practices

Terraform uses AWS IAM for provisioning.

Best practices:

Follow least privilege principle

Use separate IAM roles for:

CI/CD pipeline

Developers

Production automation

Avoid wildcards:

"Action": "*"


Use Terraform IAM modules

Example: Least Privilege S3 access

resource "aws_iam_policy" "s3_read" {
  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Action   = ["s3:GetObject"],
        Effect   = "Allow",
        Resource = ["arn:aws:s3:::mybucket/*"]
      }
    ]
  })
}

âœ… 5. Validate & Format Terraform Code

Good security is also good hygiene.

Run:
terraform fmt
terraform validate
terraform plan

Use:

âœ” tflint
âœ” tfsec (very important)
âœ” checkov
âœ” terrascan

These tools detect:

insecure security groups

open ports

IAM wildcard permissions

public S3 buckets

unencrypted EBS volumes

Example (tfsec):

tfsec

âœ… 6. Secure CI/CD Pipelines

If Terraform runs in GitHub Actions, GitLab, Jenkins:

âœ” Use OIDC federation (No static credentials!)
âœ” Store secrets in the CI vault
âœ” Restrict who can approve terraform apply

âœ… 7. Resource-Level Security Best Practices
S3 bucket:

block public access

enable encryption

enable versioning

RDS:

enable encryption

disable public access

rotate passwords via Secrets Manager

EC2:

do NOT allow 0.0.0.0/0 on port 22

use key pairs securely

Example secure SG:

ingress {
  from_port   = 22
  to_port     = 22
  protocol    = "tcp"
  cidr_blocks = [var.my_ip]   # not 0.0.0.0/0
}

âœ… 8. Terraform State Encryption

If backend is S3:

encrypt = true


For Terraform Cloud:
State is encrypted by default.

ğŸ“Œ Summary Table: Terraform Security Controls
Area				What to Secure
State File			Store remotely + encrypt
Secrets				Sensitive vars, SSM, Secrets Manager
Provider Creds			No hard-coded keys
IAM				Least privilege, avoid wildcards
Code				tflint, tfsec, checkov
Resource Security		Encryption, restrict access
CI/CD				No static secrets, approvals
Logs/Output			Hide sensitive values
****************************************************************************************


Terraform Cloud â€” TFC

Terraform Cloud (TFC) is a hosted service by HashiCorp that provides:

Remote state storage

Remote execution (apply/plan in the cloud)

Collaboration

Role-based access

Cost estimation

Private modules registry

Version control integration

Instead of running Terraform on your local system, Terraform Cloud executes the workflow for you securely and consistently.

âœ… 1. Why Use Terraform Cloud?
ğŸ”¹ Secure remote state storage

Encrypted, versioned, locked.

ğŸ”¹ Team collaboration

Multiple team members can run Terraform without conflicts.

ğŸ”¹ Runs Terraform in the cloud

Your laptop doesnâ€™t run apply.

ğŸ”¹ Integrates with GitHub / GitLab / Bitbucket

Runs Terraform automatically on pull requests.

ğŸ”¹ Role-based access control

Admins, operators, readers.

ğŸ”¹ Policy as Code (Sentinel)

Govern infrastructure changes.

ğŸ”¹ No need for S3 + DynamoDB for state

Terraform Cloud handles everything.

âœ… 2. Terraform Cloud Architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Push/Pull   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GitHub Repoâ”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Terraform Cloud   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚  (Workspace)      â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚ Runs Plan/Apply
                                        â–¼
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚ AWS / Azure â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… 3. Key Concepts
ğŸ§± Organization

Top-level container for:

Workspaces

Teams

Sentinel policies

ğŸ§± Workspaces

Equivalent to "one Terraform root module".

Examples of workspaces:

prod-vpc, dev-vpc

eks-prod

s3-buckets

Each workspace has:

Remote state

Variables

Runs (plan/apply logs)

ğŸ§± Runs

Every Terraform operation:

plan

apply

destroy

Can be triggered by:

VCS commit

Manual UI button

API

CLI

âœ… 4. Terraform Cloud Modes

Terraform Cloud can be used in two ways:

ğŸŸ¦ A) Remote Backend (most common)

You run Terraform locally, but:

State is stored in TFC

Execution happens locally or remotely (your choice)

ğŸŸ§ B) Full Remote Execution

Everything runs in TFC,
your laptop only sends commands.

*************************
Workflow 1: Terraform Cloud â€” CLI-Driven Workflow 

This is the workflow where Terraform Cloud executes your runs, but you work locally using CLI, not VCS.

ğŸ“Œ Step 1 â€” Create a Terraform Cloud Account & Organization

Go to Terraform Cloud https://app.terraform.io/

Create an organization

Create a workspace

Type â†’ CLI-driven

Your workspace will wait for runs to come from your local CLI.

ğŸ“Œ Step 2 â€” Create a Workspace (CLI-Driven)

In Terraform Cloud:

Go to Workspaces â†’ Create workspace â†’ CLI-driven

Give a name, example:
tfc-cli-demo

Done. Workspace created.

ğŸ“Œ Step 3 â€” Login from Local Machine

Run:

terraform login


This opens browser â†’ generate API token â†’ Terraform will save it at:

~/.terraform.d/credentials.tfrc.json

ğŸ“Œ Step 4 â€” Add Terraform Cloud Backend Block

Inside your project folder, create main.tf:

terraform {
  cloud {
    organization = "your-org-name"

    workspaces {
      name = "tfc-cli-demo"
    }
  }
}

provider "aws" {
  region = "us-east-1"
}

resource "aws_s3_bucket" "demo" {
  bucket = "tfc-cli-demo-bucket-123"
}

ğŸ“Œ Step 5 â€” Initialize Terraform Backend

Run:

terraform init


Now Terraform will link with Terraform Cloud workspace.

ğŸ“Œ Step 6 â€” CLI-Driven Plan (Run Happens in Cloud)

Run:

terraform plan


ğŸ“Œ Very important:
Even though you typed command locally, Terraform Cloud executes the plan.

Youâ€™ll see output like:

Terraform will perform this run remotely in Terraform Cloud.
View the run in browser? y/n


Press y â†’ browser opens.

ğŸ“Œ Step 7 â€” Apply (Remote Execution)

Run:

terraform apply


You must confirm:

Do you want to perform these actions?
only 'yes' will be accepted


Then Terraform Cloud creates resources in AWS.

ğŸ“Œ Step 8 â€” Variables Management

You DO NOT store AWS creds locally.

In Terraform Cloud workspace â†’ Variables:

Set:

Key	Value	Type
AWS_ACCESS_KEY_ID	your key	env
AWS_SECRET_ACCESS_KEY	your secret	env
AWS_REGION	us-east-1	env

Enable "Sensitive" for secret key.

Now runs work without storing secrets in your laptop.

ğŸ“Œ Step 9 â€” Terraform State Location

CLI-driven â†’ State stored in Terraform Cloud
â†’ NOT locally.

Check: in project folder no terraform.tfstate file.

State is stored here:

Terraform Cloud â†’ Workspace â†’ States

ğŸ“Œ Step 10 â€” Auto-Apply (Optional)

Workspace â†’ Settings â†’ General â†’ Enable Auto Apply

Then terraform apply will apply automatically without your confirmation.

(This is optional, usually for non-prod.)

ğŸ“Œ Step 11 â€” Destroy Resources

Run:

terraform destroy


Execution again happens in Terraform Cloud â†’ confirm â†’ resources removed.

************************************



ğŸ“Œ Workflow 2: VCS-driven (GitHub Integration)


STEP 1 â€” Create Git Repository

You can use GitHub / GitLab / Bitbucket.

1. Create a new repo

Example:
Repo name: tfc-vcs-demo

2. Add your Terraform files

Example:

main.tf

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }

  cloud {
    organization = "your-org-name"
    workspaces {
      name = "tfc-vcs-demo-ws"
    }
  }
}

provider "aws" {
  region = "us-east-1"
}

resource "aws_s3_bucket" "demo" {
  bucket = "tfc-cli-demo-bucket-123"
}


outputs.tf, variables.tf etc. as needed.

3. Push code to repo

terraform init
git add .
git commit -m "Initial commit"
git remote add origin https://github.com/Rajthetrainer/demo2
git push -u origin master


âœ… STEP 2 â€” Create Terraform Cloud Workspace (VCS-Driven)
1. Go to Terraform Cloud â†’ Workspaces â†’ Create

Click:

â¡ï¸ Create workspace
â¡ï¸ Version Control Workflow

2. Connect your VCS

Choose:

GitHub

GitLab

Bitbucket

Authorize Terraform Cloud to access your repo.

3. Select the repo

Choose the repo tfc-vcs-demo.

4. Workspace Created

Terraform Cloud now automatically:
âœ” Watches your repo
âœ” Triggers plan when code changes

âœ… STEP 3 â€” Add Workspace Variables (Secrets / Config)

Go to:

Workspace â†’ Variables

Add AWS Credentials

Category: Environment Variables

Key	Value	Sensitive
AWS_ACCESS_KEY_ID	your key	No
AWS_SECRET_ACCESS_KEY	your secret	Yes
AWS_DEFAULT_REGION	us-east-1	No

You may also add terraform variables here
(Choose â†’ Terraform Var tab)

âœ… STEP 4 â€” First Run Happens Automatically

Terraform Cloud:
âœ” Detects your repo
âœ” Runs terraform plan automatically

You will see:

Plan output

Resource changes

Cost estimation (optional)

âœ… STEP 5 â€” Approve & Apply

If Auto-Apply = OFF (default):

â†’ You must click Confirm & Apply

If Auto-Apply = ON:

â†’ Changes automatically apply on every push.

âœ… STEP 6 â€” Make Code Changes

Example: change instance size, or add a new resource.

Then:

git add .
git commit -m "Updated configuration"
git push


Terraform Cloud automatically:

âœ” Detects push
âœ” Runs new plan
âœ” Shows drift
âœ” Allows apply

âœ… STEP 7 â€” Collaboration

Team members can:

Push changes â†’ auto plan

See full history of plans/applies

Comment inside Terraform Cloud

Use notifications (Slack, email)

State is centrally stored and automatically locked during runs.

âœ… STEP 8 â€” Destroy (Optional)

To remove everything:

Terraform Cloud UI â†’
Workspace â†’ Settings â†’ Destruction and Deletion â†’ Queue Destroy Plan

Then confirm.
ğŸ’° 9. Cost Estimation

Terraform Cloud can show estimated monthly cloud costs after each plan:

+ $34.50/month (AWS EC2)


Useful before applying changes.

ğŸ” 10. Sentinel Policy (Advanced)

Allows creating security & compliance policies, like:

Disallow public S3 buckets

Restrict instance types in prod

Force tags on resources

Example:

policy "no_public_s3" {
  source = "./no_public_s3.sentinel"
  enforcement_level = "hard-mandatory"
}

ğŸ”¥ 11. Private Module Registry

Terraform Cloud lets you host private reusable modules:

registry.terraform.io/my-org/vpc/aws


Useful for enterprise infrastructure standards.

ğŸ§© 12. Terraform Cloud vs Terraform Enterprise
Terraform Cloud	 		Terraform Enterprise
SaaS (HashiCorp hosts)		Self-hosted on your servers
Free + paid tiers		Commercial only
Easy setup			Complex setup
HashiCorp maintains		Your team maintains
*********************************************************************Sentinels

Sentinel is used to enforce governance and safety rules in Terraform Cloud.

Below are the most common types of Sentinel policies, grouped by use-case:

ğŸŸ¦ 1. Cost Control Policies

Used to prevent expensive mistakes.

Examples:

Block EC2 instances larger than t3.medium

Block too many resources being created at once

Limit RDS instance class

Ensure EBS volume sizes are not too large

ğŸŸ© 2. Security Policies

Great for cloud governance.

Examples:

Deny creation of public S3 buckets

Enforce encryption on all AWS resources (S3, EBS, RDS, etc.)

Ensure security groups do not allow 0.0.0.0/0

Check IAM policies for wildcard (*) permissions

ğŸŸ¨ 3. Compliance / Tagging Policies

Most commonly used in enterprises.

Examples:

Require mandatory tags like Name, Environment, Owner

Enforce environment-specific naming conventions

Ensure all cloud resources include cost center tags

ğŸŸ§ 4. Operational Policies

Used to enforce standards and workflows.

Examples:

Prevent creation in disallowed AWS regions

Restrict Terraform version

Enforce using remote backend only

Ensure required modules (logging, monitoring) are always added

ğŸŸ¥ 5. App/Resource-Specific Policies

Examples:

Ensure KMS keys are used

Ensure CloudTrail is enabled

Ensure all S3 buckets enable access logging

Check that file content (local_file) matches requirements (your use case)

ğŸŸ« 6. Infrastructure Workflow Policies

Examples:

Block changes in production unless approved

Only allow plan but block apply unless tags match "approved"

Only allow destroy in non-production environments

ğŸ“˜ Where to find Official Sentinel Documentation?

ğŸ”µ Primary Docs
Purpose	Link
Sentinel Language (syntax)		https://docs.hashicorp.com/sentinel

Terraform Cloud Sentinel Policies	https://developer.hashicorp.com/terraform/cloud-docs/policy-enforcement

Policy Examples				https://developer.hashicorp.com/terraform/cloud-docs/policy-enforcement/examples

ğŸ§° Where to find GitHub examples?

Best open-source examples:

ğŸ”— HashiCorp Sentinel Policies Repo
https://github.com/hashicorp/terraform-guides/tree/master/governance

Contains:

AWS tag enforcement

EC2 size restrictions

S3 public access prevention

Region whitelist/blacklist

IAM wildcard block

RDS encryption rule

*********************************************
Sentinel example:

STEP 1 â€” Create Terraform project
ğŸ“„ main.tf
terraform {
  cloud {
    organization = "YOUR_ORG"

    workspaces {
      name = "sentinel-demo-ws"
    }
  }

  required_providers {
    local = {
      source  = "hashicorp/local"
      version = "~> 2.0"
    }
  }

  required_version = ">= 1.5.0"
}

provider "local" {}

resource "local_file" "demo" {
  filename = "message.txt"
  content  = "Hello Students!"
}

ğŸŸ¦ STEP 2 â€” Create .gitignore
.terraform/
terraform.tfstate*
crash.log

ğŸŸ¦ STEP 3 â€” Run locally
terraform init
terraform fmt
terraform validate


Then push:

git add .
git commit -m "sentinel demo"
git push


Terraform Cloud will trigger a run.

ğŸŸ¦ STEP 4 â€” Create Sentinel Policy in Terraform Cloud

Go to:

Org â†’ Policies â†’ Create Policy Set â†’ Upload files

Add these two files:

ğŸ“„ sentinel.hcl

(Set enforcement to hard-mandatory)

policy "file-content" {
  enforcement_level = "hard-mandatory"
}

ğŸ“„ file-content.sentinel

This policy REQUIRES that the file content contains the string:

"APPROVED"

import "tfplan/v2" as tfplan

# Get all resource changes
resources = tfplan.resource_changes

# Check local_file resources
local_file_resources = filter resources as _, r {
    r.type == "local_file"
}

# Rule: file content must include the word "APPROVED"
approved = rule {
    all local_file_resources as _, r {
        r.change.after.content contains "APPROVED"
    }
}

main = rule { approved }

ğŸŸ¥ STEP 5 â€”  FAILURE

Your current Terraform code has:

content = "Hello Students!"


This does NOT contain the word "APPROVED"
â†’ Terraform Cloud will:

âŒ Plan
âŒ Check Sentinel
âŒ FAIL with message:
"Policy file-content failed: missing required text APPROVED"


ğŸŸ© STEP 6 â€” Fix the code to PASS

Update the file content:

âœ” Update in main.tf
content = "APPROVED: Hello Students!"


Commit & push:

git add .
git commit -m "fix: added approved text"
git push


Terraform Cloud will:

âœ” Trigger new run
âœ” Plan
âœ” Sentinel check
âœ” PASS
âœ” Apply

ğŸ‰ DEMO RESULT

Bad code â†’ Sentinel blocks it

Correct code â†’ Sentinel allows it

*********************************************Examples of sentinels

1. Sentinel Policy: Mandatory AWS Tags
Policy File: enforce-tags.sentinel

import "tfplan/v2" as tfplan

required_tags = ["Name", "Environment", "Owner"]

main = rule {
    all tfplan.resource_changes as _, rc {
        rc.mode == "managed" and
        rc.change.after is map and
        (rc.change.after.tags else {}).contains_all(required_tags)
    }
}

What It Does

Forces Name, Environment, Owner tags on all resources.

If any resource misses a required tag â†’ Sentinel DENIES apply.

ğŸ’¡ Test Terraform Config 
FAIL example
resource "aws_s3_bucket" "demo" {
  bucket = "sentinel-tag-demo-123"
  # No tags â†’ FAIL
}

PASS example
resource "aws_s3_bucket" "demo" {
  bucket = "sentinel-tag-demo-123"

  tags = {
    Name        = "Demo"
    Environment = "Dev"
    Owner       = "Teacher"
  }
}

âœ… 2. Sentinel Policy: Block Public S3 Buckets
Policy File: deny-public-s3.sentinel
import "tfplan/v2" as tfplan

main = rule {
    all tfplan.resource_changes as _, rc {
        rc.type != "aws_s3_bucket" or
        (
            rc.change.after.acl != "public-read" and
            rc.change.after.acl != "public-read-write"
        )
    }
}

What It Does

Only checks aws_s3_bucket.

If ACL = public-read or public-read-write â†’ DENY.

FAIL example
resource "aws_s3_bucket" "demo" {
  bucket = "public-bucket-demo"
  acl    = "public-read"   # âŒ Will be blocked
}

PASS example
resource "aws_s3_bucket" "demo" {
  bucket = "private-bucket-demo"
  acl    = "private"
}

âœ… 3. Sentinel Policy: Restrict Allowed AWS Regions
Policy File: restrict-region.sentinel
import "tfconfig/v2" as tfconfig

allowed_regions = ["us-east-1", "us-west-2"]

main = rule {
    all tfconfig.providers.aws as _, p {
        p.config.region in allowed_regions
    }
}

What It Does

Only these regions allowed:

us-east-1

us-west-2

Anything else â†’ DENY.

FAIL example
provider "aws" {
  region = "ap-south-1"   # âŒ Blocked
}

PASS example
provider "aws" {
  region = "us-east-1"
}

***********Project with sentinel

Sentinel Policy: Block Public S3 Buckets
Policy File: deny-public-s3.sentinel
import "tfplan/v2" as tfplan

main = rule {
    all tfplan.resource_changes as _, rc {
        rc.type != "aws_s3_bucket" or
        (
            rc.change.after.acl != "public-read" and
            rc.change.after.acl != "public-read-write"
        )
    }
}

What It Does

Only checks aws_s3_bucket.

If ACL = public-read or public-read-write â†’ DENY.

Next:

main.tf
terraform {
  cloud {
    organization = "YOUR_ORG_NAME"

    workspaces {
      name = "sentinel-demo"
    }
  }

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = "us-east-1"
}

# Test resource (modify to trigger Sentinel)
resource "aws_s3_bucket" "demo" {
  bucket = "sentinel-demo-bucket-123"

  tags = {
    Name        = "Test"
    Environment = "Dev"
    Owner       = "Teacher"
  }

  # acl = "public-read"  # Uncomment to trigger failure
}